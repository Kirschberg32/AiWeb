{
 "cells": [
  {
   "cell_type": "raw",
   "id": "98b396be-e97d-460f-816d-ccf33439c478",
   "metadata": {},
   "source": [
    "Basic features\n",
    "\n",
    "1. Making Simple Requests\n",
    "\n",
    "    GET Request\n",
    "\n",
    "import requests\n",
    "response = requests.get(\"http://info.cern.ch/hypertext/WWW/TheProject.html\")\n",
    "print(response.text)  # prints the request body (here: HTML content)\n",
    "\n",
    "2. Status Codes\n",
    "\n",
    "    Check the status code of a response.\n",
    "\n",
    "print(response.status_code)  # if everything went well, this will be 200\n",
    "\n",
    "3. Response Headers\n",
    "\n",
    "    Access the response headers.\n",
    "\n",
    "headers = response.headers\n",
    "print(headers['Content-Type'])  # tells us, what kind of content we have. Should be starting with 'text/html' here\n",
    "\n",
    "4. Custom Headers\n",
    "\n",
    "    Send custom headers with the request.\n",
    "\n",
    "custom_headers = {'User-Agent': 'customAgent'}  # another could be Accept-Language (see session 1)\n",
    "response = requests.get(\"http://info.cern.ch/hypertext/WWW/TheProject.html\", headers=custom_headers)\n",
    "\n",
    "5. Timeouts\n",
    "\n",
    "    Set a timeout for the request.\n",
    "\n",
    "response = requests.get(\"http://info.cern.ch/hypertext/WWW/TheProject.html\", timeout=5)  # timeout after 5 second"
   ]
  },
  {
   "cell_type": "raw",
   "id": "146e875a-28bf-4820-aed1-90cf48902f58",
   "metadata": {},
   "source": [
    "3. Examples for the Most Fundamental Features:\n",
    "\n",
    " \n",
    "\n",
    "Parsing HTML\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# An example HTML document\n",
    "html_content = \"<html><head><title>The Title</title></head><body>Body content.</body></html>\"\n",
    "\n",
    "# The BeautifulSoup class' constructor takes an HTML document as a string and a parser name\n",
    "# We'll use the builtin 'html.parser'\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "The HTML document can (and will in all real examples) of course also come from a request!\n",
    "\n",
    " \n",
    "\n",
    "Accessing Tags\n",
    "\n",
    "# The soup object has a lot of attributes that correspond to HTML nodes (imagine the HTML document as a tree!)\n",
    "\n",
    "title_tag = soup.title\n",
    "print(title_tag)  # <title>The Title</title>\n",
    "\n",
    "# get the text content of a Tag (including possibly nested subtags' content)\n",
    "print(title_tag.text)\n",
    "\n",
    "Whenever you want to have text content without any tags, use the .text attribute.\n",
    "\n",
    " \n",
    "\n",
    "Navigating the Tree Structure\n",
    "\n",
    "print(soup.head.contents)  # [<title>The Title</title>]\n",
    "\n",
    "You can chain HTML nodes creating a path through the HTML tree.\n",
    "\n",
    " \n",
    "\n",
    "Finding Tags by Class and ID\n",
    "\n",
    "# Suppose there's a tag <div class=\"info\" id=\"main-info\">Details here</div> in our HTML\n",
    "info_div = soup.find('div', class_='info')\n",
    "main_info_div = soup.find('div', id='main-info')\n",
    "\n",
    " \n",
    "\n",
    "Access attributes of a tag\n",
    "\n",
    "# Suppose there's a tag <div class=\"info\" id=\"main-info\">Details here</div> in our HTML\n",
    "info_div = soup.find('div', class_='info')\n",
    "print(info_div['id'])\n",
    "\n",
    "The tags attrbitutes can be accessed like dictionary elements.\n",
    "\n",
    " \n",
    "\n",
    "Example: Find all headings\n",
    "\n",
    "For this, let’s assume the web content has various heading tags (h1, h2, …, h6).\n",
    "\n",
    "headings = soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])\n",
    "for heading in headings:\n",
    "    print(heading.text)\n",
    "\n",
    "By leveraging BeautifulSoup, web scraping becomes a structured and efficient task. As you dive \n",
    "deeper into its functionalities, you’ll discover a plethora of methods to refine your data extraction process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b616aee0-e316-4e1f-964b-e23d5cb7bd0a",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(html_content, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     10\u001b[0m info_div \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiv\u001b[39m\u001b[38;5;124m'\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minfo\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43minfo_div\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mid\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "# Suppose there's a tag <div class=\"info\" id=\"main-info\">Details here</div> in our HTML\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# An example HTML document\n",
    "html_content = \"<html><head><title>The Title</title></head><body>Body content.</body></html>\"\n",
    "\n",
    "# The BeautifulSoup class' constructor takes an HTML document as a string and a parser name\n",
    "# We'll use the builtin 'html.parser'\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "info_div = soup.find('div', class_='info')\n",
    "print(info_div['id'])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6b3d7c2a-c318-413a-b3b6-7ba3f461fe24",
   "metadata": {},
   "source": [
    "Part 1: The Crawler\n",
    "Algorithm:\n",
    "Initialize stack with start URL\n",
    "While stack is not empty:\n",
    "Pop first URL\n",
    "If not visited recently:\n",
    "Get the content\n",
    "Analyse it and updates the index\n",
    "Analyse it, find links to other websites and add links to stack (push)\n",
    "Update visited lst\n",
    "\n",
    "\n",
    "\n",
    "Part 2: The Index\n",
    "Algorithm:\n",
    "Dictionary:\n",
    "Terms: [ (URL1, freq, …), (URL2, freq, …), (URL3, freq, …) ]\n",
    "\n",
    "\n",
    "\n",
    "Part 3: Query parser and search algorithm\n",
    "Algorithm:\n",
    "Split the query into words:\n",
    "For each word just access the index and return the list of URL\n",
    "Return URLs that are in every list\n",
    "\n",
    "\n",
    "\n",
    "Part 4: User frontend\n",
    "• Describe different views and the flow of informat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc5586a-f6f6-4279-99ac-d68e0c0e48cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "url = \"https://vm009.rz.uos.de/crawl/index.html\"\n",
    "start = \"index.html\"\n",
    "r = requests.get(url)\n",
    "print(r.ok)\n",
    "print(\"Url:\",r.url, \"\\n\\n\")\n",
    "#print(\"Status:\",r.status_code, \"\\n\\n\")\n",
    "#print(\"Headers:\\n\",r.headers,\"\\n\\n\")\n",
    "print(\"content:\\n\",r.content,\"\\n\\n\") \n",
    "print(\"Text:\\n\",r.text,\"\\n\\n\")\n",
    "\n",
    "soup = bs(r.content,'html.parser')\n",
    "print(\"Soup: \\n\", soup.text)\n",
    "for i in soup.find_all(\"a\"):\n",
    "    if 'html' in i['href']:\n",
    "        print(i['href'])\n",
    "        r = requests.get(i['href'])\n",
    "        print(r.ok)\n",
    "        print(\"Url:\",r.url, \"\\n\\n\")\n",
    "    #else:\n",
    "    #    print(i['href'],\"is not html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406e7b55-0921-4b67-8ad3-f12acf571458",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f20e74-bdeb-42b2-9f6a-64369ca002ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict={}\n",
    "r = requests.get(\"https://vm009.rz.uos.de/crawl/index.html\")\n",
    "soup = bs(r.content,'html.parser')\n",
    "words = re.sub('[^A-Za-z0-9]+', ' ', soup.text).split()\n",
    "\n",
    "dict[\"index.html\"] = list(set(words))\n",
    "print(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c4bec1-573c-4b2c-b807-fbe6ddd89b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(html, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03707a1-c627-4a96-a6c2-196e64bc26b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"a\" in dict_1:\n",
    "    print(\"Exists\")\n",
    "else:\n",
    "    print(\"Does not exist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ab55860-3245-448b-af5a-3c70e9e1c070",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "\n",
    "class MyCrawler():\n",
    "    \n",
    "    def __init__(self, url, html):\n",
    "        try:\n",
    "            self.url = url\n",
    "            self.dict = {}\n",
    "            self.temp = [html]\n",
    "            \n",
    "            while self.temp:\n",
    "                self.__crawl__()\n",
    "        except:\n",
    "            print(\"Status: \", r.status_code, \"! Try again!\")\n",
    "\n",
    "    def __href_search__(self, soup):\n",
    "        \n",
    "        for i in soup.find_all(\"a\"):\n",
    "            if 'html' in i['href']:\n",
    "                self.temp.append(i['href'])\n",
    "\n",
    "    def __crawl__(self):\n",
    "        temp = self.temp.pop()\n",
    "        r = requests.get(self.url + temp)\n",
    "        if temp not in self.dict.keys() and r.ok:\n",
    "            print(r.status_code)\n",
    "            soup = bs(r.content,'html.parser')\n",
    "            words = re.sub('[^A-Za-z0-9]+', ' ', soup.text).split()\n",
    "            self.dict[temp] = self.url + temp ,list(set(words))\n",
    "            self.__href_search__(soup)  \n",
    "\n",
    "    def search(self):\n",
    "        words = str(input(\"Please Enter! Example: Platypus, Australia\\n\").lower())\n",
    "        words = words.split(\",\")\n",
    "        #word_list = eingabe\n",
    "        link_list = []\n",
    "        hit_list = []\n",
    "        miss_list = []\n",
    "        for html, content in a.dict.items():\n",
    "            temp_hit = []\n",
    "            temp_miss = []\n",
    "            for word in words:\n",
    "                temp_hit.append(word) if word in list(content[1]) else temp_miss.append(word)\n",
    "            link_list.append(content[0])\n",
    "            hit_list.append(temp_hit)\n",
    "            miss_list.append(temp_miss)\n",
    "        search_list = link_list,hit_list,miss_list\n",
    "        print(hit_list)\n",
    "        if not any(hit_list):\n",
    "            print(\"Nothing! Try again!\")\n",
    "            return self.search()\n",
    "        else:\n",
    "            print(\"Found something!\")\n",
    "            \n",
    "            for i in range(len(search_list[0])):\n",
    "                print(\"Url: \",search_list[0][i],\"\\nHits\", len(search_list[1][i]), \"of\", len(words), \"Found: \", hit_list[i], \"\\n\\n\")\n",
    "            return search_list\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b25add73-410e-4fbe-bfcc-6588911083c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "a = MyCrawler(\"https://vm009.rz.uos.de/crawl/\", \"index.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2be017b8-5100-4b6e-a282-32f97a63ca1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Please Enter! Example: Platypus, Australia\n",
      " platypus\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['platypus'], ['platypus'], ['platypus'], ['platypus'], [], ['platypus']]\n",
      "Found something!\n",
      "Url:  https://vm009.rz.uos.de/crawl/index.html \n",
      "Hits 1 of 1 Found:  ['platypus'] \n",
      "\n",
      "\n",
      "Url:  https://vm009.rz.uos.de/crawl/page3.html \n",
      "Hits 1 of 1 Found:  ['platypus'] \n",
      "\n",
      "\n",
      "Url:  https://vm009.rz.uos.de/crawl/page6.html \n",
      "Hits 1 of 1 Found:  ['platypus'] \n",
      "\n",
      "\n",
      "Url:  https://vm009.rz.uos.de/crawl/page2.html \n",
      "Hits 1 of 1 Found:  ['platypus'] \n",
      "\n",
      "\n",
      "Url:  https://vm009.rz.uos.de/crawl/page5.html \n",
      "Hits 0 of 1 Found:  [] \n",
      "\n",
      "\n",
      "Url:  https://vm009.rz.uos.de/crawl/page1.html \n",
      "Hits 1 of 1 Found:  ['platypus'] \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test = a.search()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "d45cf878-31bf-4330-9cc0-61edaf1d1cf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(test[0])):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9bc421de-eb3a-4141-9513-326430bf8f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index.html https://vm009.rz.uos.de/crawl/index.html\n",
      "page3.html https://vm009.rz.uos.de/crawl/page3.html\n",
      "page6.html https://vm009.rz.uos.de/crawl/page6.html\n",
      "page2.html https://vm009.rz.uos.de/crawl/page2.html\n",
      "page5.html https://vm009.rz.uos.de/crawl/page5.html\n",
      "page1.html https://vm009.rz.uos.de/crawl/page1.html\n"
     ]
    }
   ],
   "source": [
    "for i,j in a.dict.items():\n",
    "    print(i,j[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5d799ea7-2221-43d4-90d7-3ae0b90f3bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict = {\"a\":0, \"b\":1, \"c\":3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "89e87cbb-623b-44a8-ad28-75389eea4d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"a\" in dict:\n",
    "    print(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333658d6-019a-4a20-be23-fa994377f83a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbfa1a8-117d-4cdd-beb9-c9ba72c045d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
